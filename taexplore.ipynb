{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "import time\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "import re\n",
    "\n",
    "import arrow \n",
    "import os\n",
    "\n",
    "import googlemaps\n",
    "\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import cufflinks as cf\n",
    "\n",
    "import scattertext as st\n",
    "from scattertext import word_similarity_explorer\n",
    "\n",
    "from gender import GenderDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "nlp.Defaults.stop_words |= {'probably', 'perhaps', 'really', 'definitely', 'bit', 'sure'}\n",
    "# STOPWORDS = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(f):\n",
    "    @functools.wraps(f)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        t_start = time.perf_counter()\n",
    "        res = f(*args, **kwargs)\n",
    "        t_end = time.perf_counter()\n",
    "        dt = t_end - t_start\n",
    "        m, s = divmod(dt, 60)\n",
    "        print(f'[{f.__name__}] elapsed time: {m:02.0f}:{s:02.0f}')\n",
    "        return res\n",
    "    return wrapper\n",
    "\n",
    "class T:\n",
    "    \n",
    "    def __init__(self, review_file, users_file, attract_file):\n",
    "        \n",
    "        \"\"\"\n",
    "        collected TripAdvisor data comes as JSONs; this class does some data processing including imputation\n",
    "        \"\"\"\n",
    "\n",
    "        self.DIR = os.path.join('data', 'melbourne')\n",
    "        \n",
    "        self.r = json.load(open(os.path.join(self.DIR, review_file)))\n",
    "        self.u = json.load(open(os.path.join(self.DIR, users_file)))\n",
    "        self.a = json.load(open(os.path.join(self.DIR, attract_file))) \n",
    "        \n",
    "        self.attribute_encodings = json.load(open(os.path.join(self.DIR, 'attribute_encodings.json'))) \n",
    "        self.attribute_encodings_rev = defaultdict(lambda: defaultdict(str))\n",
    "        \n",
    "        self.countries = json.load(open(os.path.join(self.DIR, 'countries.json'))) \n",
    "        self.KEY_COUNTRIES = [line.lower().strip() for line in open('data/key_countries.txt').readlines() \n",
    "                                              if line.lower().strip()]\n",
    "        \n",
    "        for attr in self.attribute_encodings:\n",
    "            self.attribute_encodings_rev[attr] = {s: i for i, s in self.attribute_encodings[attr].items()}\n",
    "            \n",
    "        \n",
    "        \n",
    "#         self.gmap s = googlemaps.Client(key=open('creds/geocoding_api.key').readline().strip())\n",
    "#         self.gd = GenderDetector()\n",
    "        \n",
    "    \n",
    "    def drop_unusable(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        drop reviews or users who are not suitable for any analysis\n",
    "        \"\"\"\n",
    "        \n",
    "        self.r_df = self.r_df.dropna(subset=['text', 'attr_id'], how='any')\n",
    "        self.u_df = self.u_df.dropna(subset=['name'])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def to_pandas(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        converting all JSONs to pandas...\n",
    "        \"\"\"\n",
    "        cleanlist = lambda x: [_.strip().lower() for _ in re.sub(r'[\\[\\]\\\"\\']', '', x).strip().split(',')]\n",
    "        \n",
    "        self.u_df = pd.DataFrame(self.u)\n",
    "        self.u_df['tags'] = self.u_df['tags'].astype(str).apply(lambda x: '|'.join(cleanlist(x)) if '[' in x else None)\n",
    "\n",
    "        self.r_df = pd.DataFrame(self.r)\n",
    "        \n",
    "        self.a_df = pd.DataFrame(self.a)\n",
    "        self.a_df['cat'] = self.a_df['cat'].astype(str).apply(lambda x: '|'.join(cleanlist(x)) if '[' in x else None)\n",
    "    \n",
    "        self.genders = [_.lower() for _ in set(self.u_df['gender']) if str(_) and (str(_).lower() in 'm f'.split())]\n",
    "        \n",
    "        self.age_groups = sorted([ag for ag in set(self.u_df['age']) if '-' in str(ag)], \n",
    "                                               key=lambda x: int(str(x).split('-')[0]))\n",
    "        \n",
    "        self.tags = sorted(set(chain.from_iterable([str(s).split('|') for s in self.u_df['tags'].to_list() if '|' in str(s)])))\n",
    "        self.a_tags = sorted(set(chain.from_iterable([str(s).split('|') for s in self.a_df['cat'].to_list() if '|' in str(s)])))\n",
    "      \n",
    "        for att, n in zip(['genders', 'age groups', 'tourist tags', 'attraction tags'], \n",
    "                          [self.genders, self.age_groups, self.tags, self.a_tags]):\n",
    "            print(f'available {att}: {len(n)}')\n",
    "            print(f'{\", \".join(n)}')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def show_stats(self):\n",
    "        \n",
    "        self.stats = defaultdict()\n",
    "        \n",
    "        self.stats['users'] = len(set(self.u_df['name']))\n",
    "        self.stats['reviews'] = len(set(self.r_df['id']))\n",
    "        self.stats['attractions'] = len(set(self.a_df['id']))\n",
    "        \n",
    "        self.stats['users_with_tags'] = len(set(self.u_df[self.u_df['tags'].apply(lambda x: '|' in str(x))]['name']))\n",
    "        self.stats['users_with_gender'] = len(set(self.u_df[self.u_df['gender'].apply(lambda _: str(_) in 'm f'.split())]['name']))\n",
    "        self.stats['users_with_age'] = len(set(self.u_df[self.u_df['age'].apply(lambda _: isinstance(_, str) and bool(re.search(r'\\d+\\-\\d+', _)))]['name']))\n",
    "        \n",
    "        print(f'total users: {self.stats[\"users\"]:,}')\n",
    "        print(f'users with known gender: {self.stats[\"users_with_gender\"]:,} ({100*self.stats[\"users_with_gender\"]/self.stats[\"users\"]:.1f}%)')\n",
    "        print(f'users with known age: {self.stats[\"users_with_age\"]:,} ({100*self.stats[\"users_with_age\"]/self.stats[\"users\"]:.1f}%)')\n",
    "        print(f'users with known tags: {self.stats[\"users_with_tags\"]:,} ({100*self.stats[\"users_with_tags\"]/self.stats[\"users\"]:.1f}%)')\n",
    "               \n",
    "        return self\n",
    "    \n",
    "    def prepr_(self, review_text):\n",
    "              \n",
    "        \"\"\"\n",
    "        process a review text review_text provided as a string\n",
    "        \"\"\"\n",
    "              \n",
    "        review_ = defaultdict(list)\n",
    "\n",
    "        # create a doc from the original; this is needed to make sure the entities/labels are captured\n",
    "        # as these are sensitive to lower/upper case\n",
    "        doc = nlp(review_text)\n",
    "        # store entities and labels     \n",
    "        review_['ents'] = ' '.join([e.text for e in doc.ents]).strip()\n",
    "        review_['labels'] = ' '.join([e.label_ for e in doc.ents]).strip()\n",
    "         \n",
    "        # now create a doc from the lowercased reviews\n",
    "        doc = nlp(review_text.lower())\n",
    "        \n",
    "        review_['lemmatised'] = ' '.join([v for v in ['$' if w.is_currency else '' if w.is_stop else w.lemma_ for w in doc] if v.isalpha() and (len(v) > 1)]).strip()\n",
    "\n",
    "        review_['nouns'] = ' '.join([w.lemma_ for w in doc if w.pos_ == 'NOUN']).strip()\n",
    "        review_['verbs'] = ' '.join([w.lemma_ for w in doc if w.pos_ == 'VERB']).strip()\n",
    "        \n",
    "        return review_\n",
    "    \n",
    "    @timer\n",
    "    def process_reviews(self):\n",
    "        \n",
    "        print(f'processing {len(self.r_df):,} reviews..')\n",
    "            \n",
    "        dd = defaultdict()\n",
    "        \n",
    "        e_ = len(self.r_df)//10 \n",
    "              \n",
    "        for i, row in enumerate(self.r_df.iterrows(), 1):\n",
    "              \n",
    "            dd[row[1]['id']] = self.prepr_(row[1]['text'])\n",
    "              \n",
    "            if i%e_ == 0:\n",
    "                print(f'{100*i/len(self.r_df):3.0f}% done..')\n",
    "              \n",
    "        self.r_df = self.r_df.join(pd.DataFrame.from_dict(dd, orient='index'), on='id', how='inner')\n",
    "              \n",
    "        return self\n",
    "    \n",
    "    def save_to_csv(self, what_lst):\n",
    "        \n",
    "        if 'reviews' in what_lst:   \n",
    "            self.r_df.to_csv(os.path.join(self.DIR, 'reviews.csv'), index=False)\n",
    "            \n",
    "        if 'users' in what_lst:             \n",
    "            self.u_df.to_csv(os.path.join(self.DIR, 'users.csv'), index=False)\n",
    "            \n",
    "        if 'attractions' in what_lst:   \n",
    "            self.a_df.to_csv(os.path.join(self.DIR, 'attractions.csv'), index=False)\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def read_from_csv(self):\n",
    "        \n",
    "        self.r_df = pd.read_csv(os.path.join(self.DIR, 'reviews.csv'))\n",
    "        self.u_df = pd.read_csv(os.path.join(self.DIR, 'users.csv'))\n",
    "        self.a_df = pd.read_csv(os.path.join(self.DIR, 'attractions.csv'))\n",
    "        \n",
    "        return self\n",
    "             \n",
    "    def _tags_to_cols(self, tag_str):\n",
    "              \n",
    "        dict_ = defaultdict()\n",
    "              \n",
    "        if '|' in str(tag_str):\n",
    "              for t in tag_str.split('|'):\n",
    "                  dict_[t] = 'yes'\n",
    "              \n",
    "        return dict_\n",
    "    \n",
    "    def tags_to_cols(self):\n",
    "        \n",
    "        dict_ = defaultdict()\n",
    "              \n",
    "        for row in self.u_df.iterrows():      \n",
    "            dict_[row[1]['name']] = self._tags_to_cols(row[1]['tags'])\n",
    "              \n",
    "        self.u_df = self.u_df.drop('tags', axis=1).join(pd.DataFrame.from_dict(dict_, orient='index'), on='name', how='inner')\n",
    "        \n",
    "        dict_ = defaultdict()\n",
    "              \n",
    "        for row in self.a_df.iterrows():     \n",
    "            dict_[row[1]['id']] = self._tags_to_cols(row[1]['cat'])\n",
    "      \n",
    "        self.a_df = self.a_df.drop('cat', axis=1).join(pd.DataFrame.from_dict(dict_, orient='index'), on='id', how='inner')\n",
    "        print(self.a_df.head())\n",
    "              \n",
    "        return self\n",
    "\n",
    "              \n",
    "    def _fix_location(self, s):\n",
    "              \n",
    "        \"\"\"\n",
    "        using Google Geocoding API to clarify users location; to save on bills, we only\n",
    "        do this when there's no way to match the country to one on the list we are using\n",
    "        \"\"\"\n",
    "        \n",
    "        loc = dict()\n",
    "        \n",
    "        if not (isinstance(s, str) and s.strip()):\n",
    "            print('geocoding API needs a string argument!')\n",
    "            return loc\n",
    "        \n",
    "        geocode_result = self.gmaps.geocode(s)\n",
    "        \n",
    "        # take only the top result\n",
    "        if geocode_result:\n",
    "            res = geocode_result[0]\n",
    "        else:\n",
    "            print(f'geocoding api can\\'t find this location: {s}!')\n",
    "            return loc\n",
    "        \n",
    "        if 'address_components' in res:\n",
    "            for _ in res['address_components']:\n",
    "                if 'country' in _['types']:\n",
    "                    loc.update({'country': _['long_name']})\n",
    "                if 'locality' in _['types']:\n",
    "                    loc.update({'locality': _['long_name']})\n",
    "        if 'formatted_address' in res:\n",
    "            loc.update({'location': res['formatted_address']})\n",
    "        \n",
    "        try:\n",
    "            loc.update({'coordinates': res['geometry']['location']})\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        if not loc:\n",
    "            print('locationd fields couldn\\'t be retrieved from geocoding result!')\n",
    "                               \n",
    "        return loc\n",
    "\n",
    "    \n",
    "    def impute_location(self):\n",
    "        \n",
    "        print('imputing country...', end=' ')\n",
    "              \n",
    "        t0 = time.time()\n",
    "              \n",
    "        localities = []\n",
    "        countries = []\n",
    "    \n",
    "        c_geo = 0\n",
    "              \n",
    "        in_str = lambda s1, s2: ' ' + s1 + ' ' in ' ' + s2 + ' '\n",
    "        \n",
    "        for i, row in enumerate(self.u_df.iterrows(), 1):\n",
    "                               \n",
    "            users_country = None\n",
    "              \n",
    "            if isinstance(row[1].location, str):\n",
    "              \n",
    "                loc_str = ' '.join(re.sub(r'[\\-\\_]', ' ', row[1].location).split()).lower()\n",
    "\n",
    "                _found_countries = set()\n",
    "\n",
    "                for country in self.countries:\n",
    "              \n",
    "                    if in_str(country['name'].lower(), loc_str):\n",
    "                        _found_countries.add(country['name'].lower())\n",
    "              \n",
    "                    alt_names = country.get('other_names', None)\n",
    "              \n",
    "                    if alt_names:\n",
    "                          for alt_name in alt_names:\n",
    "                              if in_str(alt_name, loc_str):\n",
    "                                  _found_countries.add(alt_name.lower())\n",
    "\n",
    "                if len(_found_countries) == 1:\n",
    "                    users_country = _found_countries.pop()\n",
    "                else:\n",
    "#                   # run geolocation\n",
    "#                   r = self._fix_location(loc_str)\n",
    "#                   c_geo += 1\n",
    "              \n",
    "#                   if 'country' in r:\n",
    "#                      users_country = r['country'].lower()\n",
    "              \n",
    "                  users_country = None\n",
    "            \n",
    "#             print(f'#{i}: location: {row[1].location} -> country: {users_country}')\n",
    "            \n",
    "            countries.append(users_country)\n",
    "                               \n",
    "        self.u_df['country'] = [c if (not c) or (c in self.KEY_COUNTRIES) else 'other' for c in countries]\n",
    "        \n",
    "        m, s = divmod(time.time() - t0, 60)\n",
    "              \n",
    "        print(f'done. elapsed time: {m:.0f}:{s:.0f}')\n",
    "              \n",
    "#         print(f'ran geolocation {c_geo} times ({100*c_geo/len(self.u_df):.1f}%)')\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _impute_gender(self, s):\n",
    "        \n",
    "        if not isinstance(s, str):\n",
    "              return None\n",
    "              \n",
    "        # separate nicknames like TrevorJ into trevor J; or Mike23 into Mike 23\n",
    "        s = re.sub(r'([a-z]{1})([A-Z0-9]+)', r'\\1 \\2', s)\n",
    "        \n",
    "        return self.gd.gender(s)\n",
    "    \n",
    "    def impute_gender(self):\n",
    "        \n",
    "        print('imputing gender...', end=' ')\n",
    "              \n",
    "        t0 = time.time()\n",
    "        \n",
    "        avail_msk = self.u_df['gender'].str.lower().isin(self.genders)\n",
    "        \n",
    "        tot_users = len(set(self.u_df['name']))\n",
    "              \n",
    "        g_avail_bf = len(set(self.u_df[avail_msk]['name']))\n",
    "        \n",
    "        av = self.u_df[avail_msk]\n",
    "        nav = self.u_df[~avail_msk]\n",
    "        \n",
    "        nav['gender'] = nav['name'].apply(self._impute_gender)\n",
    "              \n",
    "        self.u_df = pd.concat([av, nav])\n",
    "              \n",
    "        m, s = divmod(time.time() - t0, 60)\n",
    "              \n",
    "        print(f'done. elapsed time: {m:.0f}:{s:.0f}')\n",
    "              \n",
    "        g_avail_af = len(set(self.u_df[self.u_df['gender'].str.lower().isin(self.genders)]['name']))\n",
    "              \n",
    "        print(f'availability +{100*g_avail_af/g_avail_bf - 100:.1f}%: now {g_avail_af:,} users ({100*g_avail_af/tot_users:.1f}%) was {g_avail_bf:,} ({100*g_avail_bf/tot_users:.1f}%)')\n",
    "              \n",
    "        return self\n",
    "    \n",
    "    def merge_data(self):\n",
    "        \n",
    "        self.data = self.r_df.join(self.u_df.set_index('name'), on='by_user', how='inner').rename(columns={'id': 'review_id', 'rating': ''}).join(self.a_df.set_index('id'), on='attr_id', how='inner')\n",
    "        \n",
    "        self.data.to_csv(os.path.join(self.DIR, 'data.csv'))\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def selector(self, req_dict, data_from_csv=True):\n",
    "\n",
    "        \"\"\"\n",
    "        return a data frame obtained from the original one (df) by filtering out all rows that don't match\n",
    "        the required values provided in the dictionary req_dict which looks like, for example, \n",
    "        {'age': '13-17', 'gender': 'f',...}\n",
    "\n",
    "        what if after all the filtering all that's left is an empty data frame? then just return None\n",
    "        \"\"\"\n",
    "        if data_from_csv:\n",
    "            out = pd.read_csv(os.path.join(self.DIR, 'data.csv'))\n",
    "        else:\n",
    "            out = self.data\n",
    "        \n",
    "        if self.data.empty:\n",
    "            print('dataframe you\\'re trying to select from is empty!')\n",
    "            return None\n",
    "\n",
    "        actual_cols = set(self.data.columns) | {'tag'} \n",
    "        required_cols = set(req_dict)\n",
    "\n",
    "        if not (required_cols <= actual_cols):\n",
    "            cols_na = ', '.join(required_cols - actual_cols)\n",
    "            print(f'column(s) {cols_na} you\\'re asking for are not available!')\n",
    "            return None\n",
    "\n",
    "        for col in required_cols:\n",
    "\n",
    "            if req_dict[col] != 'all':  # i.e. in {'age': '13-17'..} age value is '13-17' and not \"all\"\n",
    "\n",
    "                if col != 'tag':  \n",
    "                    # except for tags, we simply compare the actual vs required column values\n",
    "                    out = out[out[col].astype(str) == req_dict[col]]\n",
    "                else:\n",
    "                    # then it's a tag, e.g. {\"tag\": \"foodie\"}, we search where it's \"yes\" in the \"foodie\" column\n",
    "                    out = out[out[req_dict[col]] == 'yes']\n",
    "                if out.empty:\n",
    "                    break\n",
    "        \n",
    "        if out.empty:\n",
    "            return None\n",
    "        else:\n",
    "            return out\n",
    "              \n",
    "              \n",
    "    def select_reviews_for_segments(self, seg1_dict, seg2_dict, min_frq=5):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        eligible = []\n",
    "        seg_dfs = []\n",
    "              \n",
    "        for i, s in enumerate([seg1_dict, seg2_dict], 1):\n",
    "            \n",
    "            s_df = self.selector(s)\n",
    "            \n",
    "            if not isinstance(s_df, pd.DataFrame):\n",
    "                eligible.append(False)\n",
    "                break\n",
    "            \n",
    "            # what if selected not None but a data frame but there's not enough reviews?\n",
    "              \n",
    "            if len(set(s_df['by_user'])) < 50:\n",
    "                eligible.append(False)   \n",
    "            else:\n",
    "                eligible.append(True)\n",
    "                seg_dfs.append(s_df)\n",
    "        \n",
    "        \n",
    "        if not all(eligible):\n",
    "            return None\n",
    "        else:\n",
    "            \n",
    "            k1 = seg_dfs[0]\n",
    "            k1['segment'] = 'seg1'\n",
    "            k2 = seg_dfs[1]\n",
    "            k2['segment'] = 'seg2'\n",
    "            \n",
    "            kk = pd.concat([k1, k2])\n",
    "            \n",
    "                \n",
    "        return kk\n",
    "    \n",
    "    def _minmax(self, ser):\n",
    "        \n",
    "        mn_, mx_ = ser.min(), ser.max()\n",
    "        \n",
    "        return (ser - mn_)/(mx_ - mn_)\n",
    "    \n",
    "    def seg_filename(self, seg1_dict, seg2_dict):\n",
    "        \n",
    "        fn_ = ['tdf']\n",
    "        \n",
    "        for i, sdik in enumerate([seg1_dict, seg2_dict], 1):\n",
    "              \n",
    "            fn_.append('-seg' + str(i) + '-')  # so now ['tdf', '-seg1-']\n",
    "            \n",
    "            for attr in 'age gender tag country'.split():\n",
    "                    \n",
    "                fn_.append(attr[0]) # so now ['textdf', 'seg1', 'a']\n",
    "                # self.attribute_encodings_rev is like 'age': {'13-17': '1'},..\n",
    "                fn_.append(self.attribute_encodings_rev[attr] \\\n",
    "                           .get(sdik.get(attr, 'all'), 'all'))  # ['textdf', '-seg1-', 'a', '2']\n",
    "        \n",
    "        fn = os.path.join('data', ''.join(fn_) + '.csv') \n",
    "        \n",
    "        return fn\n",
    "    \n",
    "    def filename_to_seg(self, fn):\n",
    "        \n",
    "        \"\"\"\n",
    "        convert a segment data file name, e.g. tdf-seg1-a0g0t0c0-seg2-a0g0t3c0.csv to a tuple of\n",
    "        dictionaries describing segments, like ({'age': 'all'}, {'gender': 'm', 'country': 'australia'})\n",
    "        \n",
    "        recall that self.attribute_encodings is a dictionary like\n",
    "        \n",
    "        {\"age\": {\n",
    "                \"0\": \"all\",\n",
    "                \"1\": \"13-17\",\n",
    "                  }, and so on }\n",
    "        \"\"\"\n",
    "        \n",
    "        seg1_dict = defaultdict()\n",
    "        seg2_dict = defaultdict()\n",
    "        \n",
    "        for c in 'age gender tag country'.split():\n",
    "            # replace top avoid picking uip the g1 or g2 parts from seg1 or seg2\n",
    "            seg1_dict[c], seg2_dict[c] = [self.attribute_encodings[c][_] for _ in re.findall(f'(?<={c[0]})\\d+', \n",
    "                                                                                             fn.replace('seg',''))]\n",
    "        \n",
    "        \n",
    "        return (seg1_dict, seg2_dict)\n",
    "    \n",
    "    def name_from_keys(self, dict_):\n",
    "        \n",
    "        return '/'.join(sorted([k.lower() + ':' + v for k, v in dict_.items()]))\n",
    "    \n",
    "    def analyse_reviews_for_segments(self, df, seg1_dict, seg2_dict, min_frq=5):\n",
    "        \n",
    "        print(f'building corpus for segments {self.name_from_keys(seg1_dict)} and {self.name_from_keys(seg2_dict)}...')\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        corpus = st.CorpusFromPandas(df, \n",
    "                                     category_col='segment', \n",
    "                                     text_col='lemmatised', \n",
    "                                     nlp=nlp).build()\n",
    "              \n",
    "        self.freq_data = corpus.get_term_freq_df().rename(columns={'seg1 freq': 'seg1_frq', 'seg2 freq': 'seg2_frq'})\n",
    "        \n",
    "        self.freq_data['s1_score'] = corpus.get_scaled_f_scores('seg1')\n",
    "        self.freq_data['s2_score'] = corpus.get_scaled_f_scores('seg2')\n",
    "              \n",
    "        # impose min frequency\n",
    "        self.freq_data = self.freq_data[(self.freq_data['seg1_frq'] >= min_frq) & (self.freq_data['seg2_frq'] >= min_frq)]\n",
    "        print(f'{len(self.freq_data):,} words occur at least {min_frq} times')\n",
    "        \n",
    "        sc = np.vectorize(lambda s1, s2: 2*(-0.5+(s1 if s1>s2 else 1-s2 if s2>s1 else 0)))\n",
    "              \n",
    "        self.freq_data['nfsc'] = sc(self.freq_data['s1_score'], self.freq_data['s2_score'])\n",
    "              \n",
    "        # scale frequencies\n",
    "              \n",
    "        self.freq_data['seg1_frq_sc'] = self._minmax(self.freq_data['seg1_frq'])\n",
    "        self.freq_data['seg2_frq_sc'] = self._minmax(self.freq_data['seg2_frq'])\n",
    "              \n",
    "        # filename to save as .csv\n",
    "        \n",
    "        fn = self.seg_filename(seg1_dict, seg2_dict)\n",
    "\n",
    "        self.freq_data.to_csv(fn)\n",
    "              \n",
    "        m, s = divmod(time.time() - t0, 60)\n",
    "              \n",
    "        print(f'saved to {fn}. elapsed time: {m:.0f}:{s:.0f}')\n",
    "        \n",
    "        return self              \n",
    "    \n",
    "    def generate_text_dfs(self):\n",
    "        \n",
    "        genders_q = defaultdict()\n",
    "        genders_q['all'] = len(self.data)\n",
    "        for g, c in Counter(self.data['gender']).items():\n",
    "            if g in self.genders:\n",
    "                genders_q[g] = c\n",
    "                \n",
    "        ags_q = defaultdict()\n",
    "        ags_q['all'] = len(self.data)\n",
    "        for ag, c in Counter(self.data['age']).items():\n",
    "            if ag in self.age_groups:\n",
    "                ags_q[ag] = c\n",
    "        \n",
    "        tags_q = defaultdict()\n",
    "        tags_q['all'] = len(self.data)\n",
    "        for t in self.tags:\n",
    "            for yes_no, c in Counter(self.data[t]).items():\n",
    "                if (yes_no == 'yes'):\n",
    "                    tags_q[t] = c\n",
    "                       \n",
    "        genders_ext = [g for g, c in genders_q.items() if c >= 50]\n",
    "        age_groups_ext = [ag for ag, c in ags_q.items() if c >= 50]\n",
    "        tags_ext = [g for g, c in tags_q.items() if c >= 50]\n",
    "        \n",
    "        cmbs = len(genders_ext)*len(age_groups_ext)*len(tags_ext)\n",
    "        \n",
    "        print(f'total attribute combinations: {cmbs}')\n",
    "        \n",
    "        s1s = [{'age': ag, 'gender': g, 'tag': tt} for g in genders_ext\n",
    "                                                   for ag in age_groups_ext\n",
    "                                                   for tt in tags_ext]\n",
    "        s2s = [{'age': ag, 'gender': g, 'tag': tt} for g in genders_ext\n",
    "                                                   for ag in age_groups_ext \n",
    "                                                   for tt in tags_ext]\n",
    "        \n",
    "        av = 0\n",
    "        nav = 0   \n",
    "        \n",
    "        for s1 in s1s:\n",
    "            for s2 in s2s:\n",
    "                if s1 != s2:\n",
    "                    d = self.select_reviews_for_segments(s1, s2, min_frq=5)\n",
    "                    if isinstance(d, pd.DataFrame):\n",
    "                        self.analyse_reviews_for_segments(d, s1, s2, min_frq=5)\n",
    "                        av += 1\n",
    "                    else:\n",
    "                        nav += 1\n",
    "                        \n",
    "        print(f'created data files for {av} segment pairs; skipped {nav} pairs due to insufficient data')\n",
    "              \n",
    "        return self\n",
    "    \n",
    "    def pipeline(self, preprocess=True, merge=True):\n",
    "        \n",
    "        self.to_pandas()\n",
    "        \n",
    "        if preprocess:\n",
    "            \n",
    "            \"\"\"\n",
    "            pre-processing stage is when the JSON files end up as locally saved CSVs with the spaCy produced\n",
    "            columns; \n",
    "            \"\"\"\n",
    "            \n",
    "            print('pre-processing json files...')\n",
    "            \n",
    "            self.drop_unusable()\n",
    "            self.show_stats()\n",
    "            self.process_reviews()\n",
    "            self.save_to_csv(['reviews', 'attractions', 'users'])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            print('skipped pre-processing')\n",
    "            \n",
    "            self.read_from_csv()\n",
    "            self.tags_to_cols()\n",
    "            \n",
    "        \n",
    "        self.merge_data()\n",
    "        self.generate_text_dfs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available genders: 2\n",
      "m, f\n",
      "available age groups: 5\n",
      "13-17, 18-24, 25-34, 35-49, 50-64\n",
      "available tourist tags: 19\n",
      "60+ traveller, art and architecture lover, backpacker, beach goer, eco-tourist, family holiday maker, foodie, history buff, like a local, luxury traveller, nature lover, nightlife seeker, peace and quiet seeker, shopping fanatic, thrifty traveller, thrill seeker, trendsetter, urban explorer, vegetarian\n",
      "available attraction tags: 82\n",
      "airport lounges, architectural buildings, arenas & stadiums, art galleries, art museums, ballets, beaches, biking trails, bodies of water, breweries, bridges, casinos & gambling, cemeteries, childrens museums, churches & cathedrals, civic centres, concerts & shows, conference & convention centres, cultural events, department stores, events, exhibitions, factory outlets, farmers markets, farms, ferries, flea & street markets, food & drink, food & drink festivals, forests, fountains, fun & games, gardens, government buildings, hiking trails, historic sites, historic walking areas, history museums, horse tracks, libraries, lookouts, marinas, mass transportation systems, military bases & facilities, military museums, monuments & statues, museums, music festivals, national parks, nature & parks, nature & wildlife areas, neighbourhoods, observation decks & towers, observatories & planetariums, other, other zoos & aquariums, outdoor activities, piers & boardwalks, playgrounds, points of interest & landmarks, sacred & religious sites, scenic drives, scenic walking areas, science museums, shopping, shopping malls, sights & landmarks, speciality museums, sporting events, sports complexes, state parks, theatres, theme parks, tramways, transportation, traveller resources, universities & schools, visitor centres, water & amusement parks, water parks, wineries & vineyards, zoos & aquariums\n",
      "skipped pre-processing\n",
      "                                               about  \\\n",
      "0  The Melbourne Cricket Ground (MCG) is Australi...   \n",
      "1  Nothing you have ever experienced will prepare...   \n",
      "2  Visit the Shrine of Remembrance, Melbourne's m...   \n",
      "3  The National Gallery of Victoria (NGV) is the ...   \n",
      "4  Welcome to Australia's first immersive, 3D 'tr...   \n",
      "\n",
      "                                             address  \\\n",
      "0  | yarra park, jolimont, melbourne, victoria 80...   \n",
      "1  7 riverside qy | southbank, melbourne, victori...   \n",
      "2  st. kilda road, melbourne, victoria 3001, aust...   \n",
      "3  180 st kilda rd, melbourne, victoria 8004, aus...   \n",
      "4  26 star cres | level 1, the district, melbourn...   \n",
      "\n",
      "                                            attr_url         id  \\\n",
      "0  https://www.tripadvisor.com.au/Attraction_Revi...    d256584   \n",
      "1  https://www.tripadvisor.com.au/Attraction_Revi...    d654640   \n",
      "2  https://www.tripadvisor.com.au/Attraction_Revi...    d522360   \n",
      "3  https://www.tripadvisor.com.au/Attraction_Revi...    d256558   \n",
      "4  https://www.tripadvisor.com.au/Attraction_Revi...  d10062913   \n",
      "\n",
      "                             name  popularity  rank  rating  reviews  \\\n",
      "0  melbourne cricket ground (mcg)           3     1     4.5     8410   \n",
      "1                  eureka skydeck          26     2     4.5     7330   \n",
      "2           shrine of remembrance           2     3     4.5     5498   \n",
      "3    national gallery of victoria           6     4     4.5     5036   \n",
      "4                           artvo           4     5     4.5      856   \n",
      "\n",
      "                            stars  ... observatories & planetariums ferries  \\\n",
      "0   5:6167/4:1739/3:239/2:56/1:23  ...                          NaN     NaN   \n",
      "1  5:3234/4:2337/3:592/2:100/1:38  ...                          NaN     NaN   \n",
      "2     5:3747/4:1059/3:159/2:8/1:9  ...                          NaN     NaN   \n",
      "3   5:3213/4:1172/3:183/2:57/1:34  ...                          NaN     NaN   \n",
      "4        5:666/4:145/3:17/2:8/1:5  ...                          NaN     NaN   \n",
      "\n",
      "  historic walking areas monuments & statues music festivals fountains  \\\n",
      "0                    NaN                 NaN             NaN       NaN   \n",
      "1                    NaN                 NaN             NaN       NaN   \n",
      "2                    NaN                 NaN             NaN       NaN   \n",
      "3                    NaN                 NaN             NaN       NaN   \n",
      "4                    NaN                 NaN             NaN       NaN   \n",
      "\n",
      "  national parks other zoos & aquariums airport lounges scenic drives  \n",
      "0            NaN                    NaN             NaN           NaN  \n",
      "1            NaN                    NaN             NaN           NaN  \n",
      "2            NaN                    NaN             NaN           NaN  \n",
      "3            NaN                    NaN             NaN           NaN  \n",
      "4            NaN                    NaN             NaN           NaN  \n",
      "\n",
      "[5 rows x 92 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index(['rating'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-8766af88425f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0musers_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'usrs3.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m              attract_file='attractions_melbourne.json') \\\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-68-12647923c068>\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(self, preprocess, merge)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_text_dfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-12647923c068>\u001b[0m in \u001b[0;36mmerge_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmerge_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'by_user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'review_id'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'attr_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6813\u001b[0m         \u001b[0;31m# For SparseDataFrame's benefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6814\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[0;32m-> 6815\u001b[0;31m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[1;32m   6816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6817\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6828\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[1;32m   6829\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6830\u001b[0;31m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[1;32m   6831\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6832\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     46\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                          validate=validate)\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         llabels, rlabels = items_overlap_with_suffix(ldata.items, lsuf,\n\u001b[0;32m--> 552\u001b[0;31m                                                      rdata.items, rsuf)\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mlindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mleft_indexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mitems_overlap_with_suffix\u001b[0;34m(left, lsuffix, right, rsuffix)\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlsuffix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m             raise ValueError('columns overlap but no suffix specified: '\n\u001b[0;32m-> 1972\u001b[0;31m                              '{rename}'.format(rename=to_rename))\n\u001b[0m\u001b[1;32m   1973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlrenamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: columns overlap but no suffix specified: Index(['rating'], dtype='object')"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    t = T(review_file='revs3.json',\n",
    "             users_file='usrs3.json',\n",
    "             attract_file='attractions_melbourne.json') \\\n",
    "            .pipeline(preprocess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAKE = 300\n",
    "\n",
    "df = pd.read_csv('data/tdf-seg1-a0g0t0c0-seg2-a0g0t5c0.csv')\n",
    "\n",
    "s1_tb, s1_tu = df['seg1_frq_sc'].quantile(q=[0.20, 0.95])\n",
    "s2_tb, s2_tu = df['seg2_frq_sc'].quantile(q=[0.20, 0.95])\n",
    "\n",
    "df = df[((df['seg1_frq_sc'] < s1_tu) & (df['seg1_frq_sc'] > s1_tb)) & \n",
    "        ((df['seg2_frq_sc'] < s2_tu) & (df['seg2_frq_sc'] > s2_tb))].iloc[:TAKE]\n",
    "\n",
    "colorscale=[[0, 'orange'], [1, '#2C72EC']]\n",
    "\n",
    "layout= go.Layout(\n",
    "#     title= 'Characteristic Words',\n",
    "    hovermode= 'closest',\n",
    "    xaxis= dict(\n",
    "        title='Frequency in Reviews by Seg 1',\n",
    "        ticklen= 5,\n",
    "        tickmode='array',\n",
    "        tickvals=np.linspace(df['seg1_frq_sc'].min(), df['seg1_frq_sc'].max(), num=5),\n",
    "        ticktext=['low', '', 'medium', '', 'high'],\n",
    "        zeroline= False,\n",
    "        gridwidth= 2,\n",
    "        showticklabels=True,\n",
    "        showgrid=True,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        ticklen= 5,\n",
    "        tickmode='array',\n",
    "        tickvals=np.linspace(df['seg2_frq_sc'].min(), df['seg2_frq_sc'].max(), num=5),\n",
    "        ticktext=['low', '', 'medium', '', 'high'],\n",
    "        gridwidth= 2,\n",
    "        zeroline=False,\n",
    "        showticklabels=True,\n",
    "        showgrid=True,\n",
    "        tickangle=-90,\n",
    "        title='Frequency in Reviews by Seg 2',\n",
    "    ),\n",
    "    legend=dict(orientation=\"h\", x=0.5, y=1.1, yanchor=\"top\"),\n",
    "    annotations=[dict(text='Stronger Association with ', x=0.41, y=1.08, \n",
    "            showarrow=False, \n",
    "            xref=\"paper\",\n",
    "            yref=\"paper\",\n",
    "            yanchor=\"top\"\n",
    "                     )],\n",
    "    showlegend= True\n",
    ")\n",
    "\n",
    "df_neg = df[df['nfsc']<0]\n",
    "df_pos = df[df['nfsc']>0]\n",
    "\n",
    "trace0 = go.Scatter(\n",
    "    name='Seg1',\n",
    "    x = df_neg['seg1_frq_sc'],\n",
    "    y = df_neg['seg2_frq_sc'],\n",
    "    mode = 'markers',\n",
    "    hoverinfo='text', \n",
    "    marker=dict(\n",
    "                color='orange', \n",
    "                size=10,\n",
    "                opacity=0.85,\n",
    "               ),\n",
    "    text= df_neg.term)\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    name='Seg2',\n",
    "    x = df_pos['seg1_frq_sc'],\n",
    "    y = df_pos['seg2_frq_sc'],\n",
    "    mode = 'markers',\n",
    "    hoverinfo='text', \n",
    "    marker=dict(\n",
    "                color='#2C72EC', \n",
    "                size=10, \n",
    "                opacity=0.85,\n",
    "               ),\n",
    "    text= df_pos.term)\n",
    "\n",
    "\n",
    "fig= go.Figure(data=[trace0, trace1], layout=layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "trace2 = go.Scatter(\n",
    "    name='Seg2',\n",
    "    x = random.choices(np.linspace(0,1), k=len(df_pos['term'])),\n",
    "    y = df_pos['seg2_frq_sc'],\n",
    "    mode = 'text',\n",
    "    hoverinfo='text', \n",
    "    marker=dict(\n",
    "                color='#2C72EC', \n",
    "                size=10, \n",
    "                opacity=0.85\n",
    "               ),\n",
    "    textfont={'size': df_pos['seg1_frq']/df_pos['seg1_frq'].max()*40,\n",
    "                           'color': 'orange'},\n",
    "    text= df_pos.term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2= go.Figure(data=[trace2])\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, width=300).generate(' '.join(df_pos.term.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(wordcloud, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot(fig2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
